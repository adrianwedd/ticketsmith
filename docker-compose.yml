version: '3.9'
services:
  agent:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ticketsmith_agent
    environment:
      - INFERENCE_URL=http://inference:8000
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
    volumes:
      - .:/app
    depends_on:
      - inference
      - qdrant
  inference:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: llm_inference
    ports:
      - "8000:8000"
    environment:
      - VLLM_MODEL=${VLLM_MODEL:-meta-llama/Llama-3-8B-Instruct}
      - INFERENCE_API_KEY=${INFERENCE_API_KEY:-changeme}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  qdrant:
    image: qdrant/qdrant:v1.7.3
    container_name: qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
volumes:
  qdrant_data:
